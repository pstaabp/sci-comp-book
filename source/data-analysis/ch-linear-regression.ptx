<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch-linear-regression">
  <title>Linear Regression</title>

  <introduction>
    <p>
      One common task when having a dataset is to build a model of the data and a common model is a linear one. Consider the <c>cars</c> dataset in the <c>RDatasets</c> package.  This dataset is a simple two column set with the speed a car is travelling and the distance it takes to reach a full stop.  A scatter plot of the data is:
    </p>

    <figure xml:id="fig-lr-cars-dist">
      <caption></caption>
      <image source="plots/linear-regression/speed-dist.png" width="90%">
        <shortdescription>
          (for accessibility)
        </shortdescription>
      </image>
    </figure>

    <p>
      This plot was created by loading in <c>RDatasets, CairoMakie</c>, activating <c>CairoMakie</c> and then
    </p>


    <program language="julia" line-numbers="yes">
      <input>
cars=RDatasets.dataset("datasets","cars")
fig = Figure()
ax = Axis(fig[1,1], xlabel="Speed of Car", ylabel = "Distance to Stop")
scatter!(ax, cars.Speed, cars.Dist)
fig
      </input>
    </program>

    <p>
      The plot reveals that the faster the car is traveling, the longer it takes to stop.  This probably isn't a surprise, but perhaps we'd like to know the relationship between then.  That is, can we predict the stopping distance if we know the speed of the car.  The simplest model to use for this is a linear one.
    </p>
  </introduction>

  <section>
    <title>Basics of Linear Regression</title>

    <p>
      Before finding the linear model for the above example, let's look at a simpler dataset.  Consider
    </p>

    <p>
      <cd>
      <cline>data = DataFrame(x=[1,3,4,6,7,9, 10], y = [10, 9, 7, 6, 5, 4, 2])</cline>
      </cd>
    </p>

    <p>
      and has the following scatter plot:
    </p>

    <figure xml:id="fig-lr-simple">
      <caption></caption>
      <image source="plots/linear-regression/simple-scatter.png" width="90%">
        <shortdescription>
          (for accessibility)
        </shortdescription>
      </image>
    </figure>

    <p>
      We seek a line of the form <m>y=ax+b</m> which we plot below and for each data value, we define <m>\epsilon_k = y_k - (a x_k+b)</m>, which is the vertical signed distance between the line and the data as shown below:
    </p>

    <figure xml:id="fig-lr-diagram">
      <caption></caption>
      <image source="plots/linear-regression/lr-diagram.png" width="90%">
        <shortdescription>
          (for accessibility)
        </shortdescription>
      </image>
    </figure>

    <p>
      We then find the total square distance of all of the errors as
    </p>

    <p>
      <men xml:id="eqn-sum-sq">
        S(a,b) = \sum_{k=1}^n (y_k - (ax_k+b))^2
      </men>
    </p>

    <p>
      which is explicitly stated as a function of <m>a</m> and <m>b</m>, the slope and <m>y</m>-intercept of the line.  The best fine line then is the line that minimizes this function. To find the values of <m>a</m> and <m>b</m>, there are many techniques to accomplish this<fn>A common way to do this is to use techniques from multi-variate calculus that differentiates the function then finds the value where the derivatives are 0.</fn>, here we will write this as
    </p>

    <p>
      <md>
        <mrow>X \amp = \begin{bmatrix} 1 &amp; x_1 \\ 1 &amp; x_2 \\ \vdots \\ 1 &amp; x_n \end{bmatrix} \amp \amp \vec{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}  \amp \vec{c} \amp \begin{bmatrix} a \\ b \end{bmatrix} </mrow>
      </md>
    </p>

    <p>
      and then <xref ref="eqn-sum-sq"/> can be written as
    </p>

    <p>
      <md>
        <mrow>S(a,b) \amp = (X \vec{c}  - \vec{y})^T (X \vec{c}-\vec{y})</mrow>
      </md>
    </p>

    <p>
      and differentiating with <m>\partial_{\vec{x}} [(\vec{F}(\vec{x}))^{\intercal} \vec{F}(\vec{x})] = 2 \vec{F}(\vec{x})^{\intercal} \vec{F}(\vec{x})</m> leads to
    </p>

    <p>
      <md>
        <mrow>\frac{\partial S}{\partial \vec{c}} \amp =  2 X^{\intercal} (X \vec{c} - \vec{y})</mrow>
        <mrow> \amp = 2 X^{\intercal} X \vec{c} - 2 X \vec{y} </mrow>
      </md>
    </p>

    <p>
      and setting this to <m>\vec{0}</m> and solving for <m>\vec{c}</m> results in
    </p>

    <p>
      <me>
        \vec{c} = (X^{\intercal} X)^{-1} X \vec{y}
      </me>
    </p>

    <p>
      We now apply this to the above example.  First, we will create the <c>X</c> matrix with the following:
    </p>

    <p>
      <cd>
      <cline>X = [data.x[i]^k for i=1:size(data,1), k=0:1]</cline>
      </cd>
    </p>

    <p>
      which generates the matrix with ones in the first column and <c>data.x</c> in the second.  And then
    </p>

    <p>
      <cd>
      <cline>c = inv(transpose(X)*X)*transpose(X)*data.y</cline>
      </cd>
    </p>

    <p>
      results in the vector <c>[ 10.982, -0.8468]</c>, which is the vector of <m>y</m>-intercept and slope.  So the best-fit line for this is <m>y=-0.8468x+10.982</m>.  We can use this to plot the data and the best-fit line as follows:
    </p>

    <p>
      <cd>
      <cline>fig, ax = scatter(data.x,data.y)</cline>
      <cline>lines!(ax,0..10,x -&gt; c[2]*x+c[1], color = :darkgreen)</cline>
      <cline>fig</cline>
      </cd>
    </p>

    <p>
      which produces the following plot:
    </p>

    <figure xml:id="fig-lr-bestfit">
      <caption></caption>
      <image source="plots/linear-regression/best-fit.png" width="90%">
        <shortdescription>
          (for accessibility)
        </shortdescription>
      </image>
    </figure>

    <p>
      This looks good visually, but how do we know it is a good fit.  There are a number of parameters that we can use to calculate the fit, but instead, we'll switch to using a package.
    </p>
  </section>

  <section>
    <title>Linear Regression with GLM package</title>

    <p>
      There is a robust packge for regression called <c>GLM</c> short for Generalized Linear Models, which allows many different kinds of regression performed on a dataset.  Make sure that you have added this package and loaded it with <c>using GLM</c>.
    </p>

    <p>
      To start with a simple model, we can produce a best fit line with the formula <c>y ~ x</c>, meaning that we want <c>y</c>, which is a column in our <c>data</c> Dataframe as the dependent variable to be linear in <c>x</c> the column that will be the independent variable.  After creating a formula with the <c>@formula</c> macro, we fit the data with the <c>lm</c> short for linear model method as in:
    </p>

    <p>
      <cd>
      <cline>fm = @formula(y ~ x)</cline>
      <cline>model = lm(fm, data)</cline>
      </cd>
    </p>

    <p>
      The result is
    </p>

    <p>
      <cd>
      <cline>y ~ 1 + x</cline>
      <cline></cline>
      <cline>Coefficients:</cline>
      <cline>──────────────────────────────────────────────────────────────────────────</cline>
      <cline>                 Coef.  Std. Error       t  Pr(&gt;|t|)  Lower 95%  Upper 95%</cline>
      <cline>──────────────────────────────────────────────────────────────────────────</cline>
      <cline>(Intercept)  10.982      0.4244      25.88    &lt;1e-05    9.89103  12.0729</cline>
      <cline>x            -0.846847   0.0657102  -12.89    &lt;1e-04   -1.01576  -0.677933</cline>
      <cline>──────────────────────────────────────────────────────────────────────────</cline>
      </cd>
    </p>

    <p>
      There are a number of things to note here:
    </p>

    <p>
      <ul>
        <li>
          <p>
            The top line is the formula <c>y ~ 1 + x</c> which is the same as <c>y ~ x</c>.  The <c>1</c> denotes that you want to include a constant term in your formula.  Note: you can do <c>y ~ 0 + x</c> to force no constant term.
          </p>
        </li>

        <li>
          <p>
            The cofficients table has information about all of the coefficients, in this case 2, the constant term and the coefficient of the <c>x</c> term.  The first column is which coefficient and the second is the best-fit value of this coefficient.  Notice that this is the same as we found above using the derived formulas.
          </p>
        </li>

        <li>
          <p>
            The remainder of the table indicate statistics to determine the significance of each of the coefficients.  The <c>Pr(&gt; |t|)</c> is the p-value for each of the coefficients differs from 0.  In this case, each are highly significant due to the small values.  The last two columns give a confidence interval.  To 95% confidence, each coefficient is between the last two columns.
          </p>
        </li>
      </ul>
    </p>

    <p>
      Additionally, we can determine the overall fit of the model with an <m>r^2</m> value.  This is accomplished with <c>r2(model)</c> which returns <c>0.970775653702483</c>.  A standard interpretation of this is that the linear model explains 97% of the variation in the data.
    </p>
  </section>
</chapter>
