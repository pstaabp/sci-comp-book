<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch-linear-regression">
  <title>Linear Regression</title>

  <introduction>
    <p>
      One common task when having a dataset is to build a model of the data and a common model is a linear one. Consider the <c>cars</c> dataset in the <c>RDatasets</c> package.  This dataset is a simple two column set with the speed a car is travelling and the distance it takes to reach a full stop.  A scatter plot of the data is:
    </p>

    <figure xml:id="fig-lr-cars-dist">
      <caption></caption>
      <image source="plots/linear-regression/speed-dist.png" width="90%">
        <shortdescription>
          (for accessibility)
        </shortdescription>
      </image>
    </figure>

    <p>
      This plot was created by loading in <c>RDatasets, CairoMakie</c>, activating <c>CairoMakie</c> and then
    </p>


    <program language="julia" line-numbers="yes">
      <input>
cars=RDatasets.dataset("datasets","cars")
fig = Figure()
ax = Axis(fig[1,1], xlabel="Speed of Car", ylabel = "Distance to Stop")
scatter!(ax, cars.Speed, cars.Dist)
fig
      </input>
    </program>

    <p>
      The plot reveals that the faster the car is traveling, the longer it takes to stop.  This probably isn't a surprise, but perhaps we'd like to know the relationship between then.  That is, can we predict the stopping distance if we know the speed of the car.  The simplest model to use for this is a linear one.
    </p>
  </introduction>

  <section>
    <title>Basics of Linear Regression</title>

    <p>
      Before finding the linear model for the above example, let's look at a simpler dataset.  Consider
    </p>

    <p>
      <cd>
      <cline>data = DataFrame(x=[1,3,4,6,7,9, 10], y = [10, 9, 7, 6, 5, 4, 2])</cline>
      </cd>
    </p>

    <p>
      and has the following scatter plot:
    </p>

    <figure xml:id="fig-lr-simple">
      <caption></caption>
      <image source="plots/linear-regression/simple-scatter.png" width="90%">
        <shortdescription>
          (for accessibility)
        </shortdescription>
      </image>
    </figure>

    <p>
      We seek a line of the form <m>y=ax+b</m> which we plot below and for each data value, we define <m>\epsilon_k = y_k - (a x_k+b)</m>, which is the vertical signed distance between the line and the data as shown below:
    </p>

    <figure xml:id="fig-lr-diagram">
      <caption></caption>
      <image source="plots/linear-regression/lr-diagram.png" width="90%">
        <shortdescription>
          (for accessibility)
        </shortdescription>
      </image>
    </figure>

    <p>
      We then find the total square distance of all of the errors as
    </p>

    <p>
      <men xml:id="eqn-sum-sq">
        S(a,b) = \sum_{k=1}^n (y_k - (ax_k+b))^2
      </men>
    </p>

    <p>
      which is explicitly stated as a function of <m>a</m> and <m>b</m>, the slope and <m>y</m>-intercept of the line.  The best fine line then is the line that minimizes this function. To find the values of <m>a</m> and <m>b</m>, there are many techniques to accomplish this<fn>A common way to do this is to use techniques from multi-variate calculus that differentiates the function then finds the value where the derivatives are 0.</fn>, here we will write this as
    </p>

    <p>
      <md>
        <mrow>X \amp = \begin{bmatrix} 1 &amp; x_1 \\ 1 &amp; x_2 \\ \vdots \\ 1 &amp; x_n \end{bmatrix} \amp \amp \vec{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}  \amp \vec{c} \amp \begin{bmatrix} a \\ b \end{bmatrix} </mrow>
      </md>
    </p>

    <p>
      and then <xref ref="eqn-sum-sq"/> can be written as
    </p>

    <p>
      <md>
        <mrow>S(a,b) \amp = (X \vec{c}  - \vec{y})^T (X \vec{c}-\vec{y})</mrow>
      </md>
    </p>

    <p>
      and differentiating with <m>\partial_{\vec{x}} [(\vec{F}(\vec{x}))^{\intercal} \vec{F}(\vec{x})] = 2 \vec{F}(\vec{x})^{\intercal} \vec{F}(\vec{x})</m> leads to
    </p>

    <p>
      <md>
        <mrow>\frac{\partial S}{\partial \vec{c}} \amp =  2 X^{\intercal} (X \vec{c} - \vec{y})</mrow>
        <mrow> \amp = 2 X^{\intercal} X \vec{c} - 2 X \vec{y} </mrow>
      </md>
    </p>

    <p>
      and setting this to <m>\vec{0}</m> and solving for <m>\vec{c}</m> results in
    </p>

    <p>
      <me>
        \vec{c} = (X^{\intercal} X)^{-1} X \vec{y}
      </me>
    </p>

    <p>
      We now apply this to the above example.  First, we will create the <c>X</c> matrix with the following:
    </p>

    <p>
      <cd>
      <cline>X = [data.x[i]^k for i=1:size(data,1), k=0:1]</cline>
      </cd>
    </p>

    <p>
      which generates the matrix with ones in the first column and <c>data.x</c> in the second.  And then
    </p>

    <p>
      <cd>
      <cline>c = inv(transpose(X)*X)*transpose(X)*data.y</cline>
      </cd>
    </p>

    <p>
      results in the vector <c>[ 10.982, -0.8468]</c>, which is the vector of <m>y</m>-intercept and slope.  So the best-fit line for this is <m>y=-0.8468x+10.982</m>.  We can use this to plot the data and the best-fit line as follows:
    </p>

    <p>
      <cd>
      <cline>fig, ax = scatter(data.x,data.y)</cline>
      <cline>lines!(ax,0..10,x -&gt; c[2]*x+c[1], color = :darkgreen)</cline>
      <cline>fig</cline>
      </cd>
    </p>
    <p>
      which produces the following plot:
    </p>

    <figure xml:id="fig-lr-bestfit">
      <caption></caption>
      <image source="plots/linear-regression/best-fit.png" width="90%">
        <shortdescription>
          (for accessibility)
        </shortdescription>
      </image>
    </figure>
  </section>
</chapter>
